spring.application.name=spring-ollama-vector
# Ollama Configuration
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=mistral
spring.ai.ollama.chat.options.temperature=0.7
#embeding model
spring.ai.model.embedding=ollama
spring.ai.ollama.embedding.options.model=nomic-embed-text
# Server Configuration
server.port=8080
# Logging Configuration
logging.level.org.springframework.ai=DEBUG
logging.level.com.dburnwal.springai=DEBUG